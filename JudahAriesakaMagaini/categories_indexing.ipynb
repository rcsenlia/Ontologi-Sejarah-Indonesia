{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class dan function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdMap:\n",
    "    def __init__(self):\n",
    "        self.str_to_id = {}\n",
    "        self.id_to_str = []\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO\n",
    "        return len(self.id_to_str)\n",
    "\n",
    "    def __get_id(self, s):\n",
    "        # TODO\n",
    "        if s not in self.str_to_id.keys():\n",
    "            self.str_to_id[s] = len(self.id_to_str)\n",
    "            self.id_to_str.append(s)\n",
    "\n",
    "        return self.str_to_id[s]\n",
    "\n",
    "    def __get_str(self, i):\n",
    "        # TODO\n",
    "        return self.id_to_str[i]\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        # TODO\n",
    "        if isinstance(key, int):\n",
    "            return self.__get_str(key)\n",
    "        elif isinstance(key, str):\n",
    "            return self.__get_id(key)\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "def sort_union_list(list_A, list_B):\n",
    "    res = []\n",
    "    p1 = p2 = 0\n",
    "    max1 = len(list_A)\n",
    "    max2 = len(list_B)\n",
    "\n",
    "    while p1 < max1 and p2 < max2:\n",
    "        if list_A[p1] == list_B[p2]:\n",
    "            res.append(list_A[p1])\n",
    "            p1 += 1\n",
    "            p2 += 1\n",
    "        elif list_A[p1] < list_B[p2]:\n",
    "            res.append(list_A[p1])\n",
    "            p1 += 1\n",
    "        else:\n",
    "            res.append(list_B[p2])\n",
    "            p2 += 1\n",
    "            \n",
    "    while p1 < max1:\n",
    "        res.append(list_A[p1])\n",
    "        p1 += 1\n",
    "\n",
    "    while p2 < max2:\n",
    "        res.append(list_B[p2])\n",
    "        p2 += 1\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array\n",
    "\n",
    "class VBEPostings:\n",
    "\n",
    "    @staticmethod\n",
    "    def encode(postings_list):\n",
    "        # TODO\n",
    "\n",
    "        if not postings_list:\n",
    "            return array.array('B', []).tobytes()\n",
    "\n",
    "        gap_based = [postings_list[0]]\n",
    "        for i in range(1, len(postings_list)):\n",
    "            gap_based.append(postings_list[i] - postings_list[i - 1])\n",
    "\n",
    "        # print(gap_based)\n",
    "\n",
    "        return VBEPostings.vb_encode(gap_based)\n",
    "\n",
    "    @staticmethod\n",
    "    def vb_encode(list_of_numbers):\n",
    "        # TODO\n",
    "        bytestream = array.array('B')\n",
    "        for number in list_of_numbers:\n",
    "            byte = VBEPostings.vb_encode_number(number)\n",
    "            bytestream.extend(byte)\n",
    "\n",
    "        # print(bytestream)\n",
    "\n",
    "        return bytestream.tobytes()\n",
    "\n",
    "    @staticmethod\n",
    "    def vb_encode_number(number):\n",
    "        # TODO\n",
    "        byte = []\n",
    "        while True:\n",
    "            byte.insert(0, number % 128)\n",
    "            if number < 128:\n",
    "                break\n",
    "            number = number // 128\n",
    "        byte[-1] += 128\n",
    "\n",
    "        # print(byte)\n",
    "\n",
    "        return array.array('B', byte).tobytes()\n",
    "\n",
    "    @staticmethod\n",
    "    def decode(encoded_postings_list):\n",
    "        # TODO\n",
    "        if not encoded_postings_list:\n",
    "            return []\n",
    "\n",
    "        decoded = VBEPostings.vb_decode(encoded_postings_list)\n",
    "        postings_list = [decoded[0]]\n",
    "        for i in range(1, len(decoded)):\n",
    "            postings_list.append(decoded[i] + postings_list[i - 1])\n",
    "\n",
    "        return postings_list\n",
    "\n",
    "    @staticmethod\n",
    "    def vb_decode(encoded_bytestream):\n",
    "        \"\"\"\n",
    "        Decoding sebuah bytestream yang sebelumnya di-encode dengan\n",
    "        variable-byte encoding.\n",
    "        \"\"\"\n",
    "        numbers = []\n",
    "        n = 0\n",
    "        for byte in encoded_bytestream:\n",
    "            if byte < 128:\n",
    "                n = 128 * n + byte\n",
    "            else:\n",
    "                n = 128 * n + byte - 128\n",
    "                numbers.append(n)\n",
    "                n = 0\n",
    "        return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "class InvertedIndex:\n",
    "    def __init__(self, index_name, encoding_method, path=''):\n",
    "        self.encoding_method = encoding_method\n",
    "        self.path = path\n",
    "\n",
    "        self.index_file_path = os.path.join(path, index_name + '.index')\n",
    "        self.metadata_file_path = os.path.join(path, index_name + '.dict')\n",
    "\n",
    "        self.postings_dict = {}\n",
    "        self.cats = []  # Untuk keep track urutan cat yang dimasukkan ke index\n",
    "\n",
    "    def __enter__(self):\n",
    "        # Membuka index file\n",
    "        self.index_file = open(self.index_file_path, 'rb+')\n",
    "\n",
    "        # Kita muat postings dict dan cats iterator dari file metadata\n",
    "        with open(self.metadata_file_path, 'rb') as f:\n",
    "            self.postings_dict, self.cats = pickle.load(f)\n",
    "            self.cat_iter = self.cats.__iter__()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exception_type, exception_value, traceback):\n",
    "        # Menutup index file\n",
    "        self.index_file.close()\n",
    "\n",
    "        # Menyimpan metadata (postings dict dan cats) ke file metadata dengan bantuan pickle\n",
    "        with open(self.metadata_file_path, 'wb') as f:\n",
    "            pickle.dump([self.postings_dict, self.cats], f)\n",
    "\n",
    "\n",
    "class InvertedIndexReader(InvertedIndex):\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self.index_file.seek(0)\n",
    "        self.cat_iter = self.cats.__iter__()  # reset cat iterator\n",
    "\n",
    "    def __next__(self):\n",
    "        # TODO\n",
    "        try:\n",
    "            cat = next(self.cat_iter)\n",
    "            return cat, self.get_postings_list(cat)\n",
    "        except Exception:\n",
    "            self.reset()\n",
    "            raise StopIteration\n",
    "\n",
    "    def get_postings_list(self, cat):\n",
    "        # TODO\n",
    "        start_position_in_index_file, _, length_in_bytes_of_postings_list = self.postings_dict[cat]\n",
    "\n",
    "        self.index_file.seek(start_position_in_index_file)\n",
    "        encoded_posting_lists = self.index_file.read(length_in_bytes_of_postings_list)\n",
    "        posting_lists = self.encoding_method.decode(encoded_posting_lists)\n",
    "\n",
    "        return posting_lists\n",
    "\n",
    "\n",
    "class InvertedIndexWriter(InvertedIndex):\n",
    "    def __enter__(self):\n",
    "        self.index_file = open(self.index_file_path, 'wb+')\n",
    "        return self\n",
    "\n",
    "    def append(self, cat, postings_list):\n",
    "        # TODO\n",
    "        encoded_posting_lists = self.encoding_method.encode(postings_list)\n",
    "\n",
    "        self.index_file.seek(0, 2) # end of file\n",
    "        start_position_in_index_file = self.index_file.tell()\n",
    "        number_of_postings_in_list = len(postings_list)\n",
    "        length_in_bytes_of_postings_list = len(encoded_posting_lists)\n",
    "\n",
    "        self.postings_dict[cat] = (\n",
    "            start_position_in_index_file,\n",
    "            number_of_postings_in_list,\n",
    "            length_in_bytes_of_postings_list\n",
    "        )\n",
    "\n",
    "        self.index_file.write(encoded_posting_lists)\n",
    "        self.cats.append(cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memindahkan seluruh page ke CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'idwiki-20240201-pages-articles.xml.bz2'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bz2\n",
    "\n",
    "data_path = \"idwiki-20240201-pages-articles.xml.bz2\"\n",
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.sax\n",
    "\n",
    "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
    "    \"\"\"Content handler for Wiki XML data using SAX\"\"\"\n",
    "    def __init__(self):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buffer = None\n",
    "        self._values = {}\n",
    "        self._current_tag = None\n",
    "        self._pages = []\n",
    "\n",
    "    def characters(self, content):\n",
    "        \"\"\"Characters between opening and closing tags\"\"\"\n",
    "        if self._current_tag:\n",
    "            self._buffer.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        \"\"\"Opening tag of element\"\"\"\n",
    "        if name in ('title', 'text', 'timestamp'):\n",
    "            self._current_tag = name\n",
    "            self._buffer = []\n",
    "\n",
    "    def endElement(self, name):\n",
    "        \"\"\"Closing tag of element\"\"\"\n",
    "        if name == self._current_tag:\n",
    "            self._values[name] = ' '.join(self._buffer)\n",
    "\n",
    "        if name == 'page':\n",
    "            self._pages.append((self._values['title'], self._values['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = WikiXmlHandler()\n",
    "\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "for line in bz2.BZ2File(data_path, 'r'):\n",
    "    parser.feed(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('pages.csv', 'w', newline='', encoding='UTF-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    writer.writerow([\"ID\", \"Title\", \"Text\"])\n",
    "    \n",
    "    for i, row in enumerate(handler._pages):\n",
    "        writer.writerow([i] + list(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mendapatkan kategori untuk setiap halaman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mwparserfromhell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kategoris(row):\n",
    "    wiki = mwparserfromhell.parse(row[2])\n",
    "    wikilinks = wiki.filter_wikilinks(matches=\"\\[\\[Kategori:\")\n",
    "    # print(wikilinks)\n",
    "    kategoris_links = [kat.lower().split('kategori:')[1].split(']')[0] for kat in wikilinks]\n",
    "    \n",
    "    kategoris = [kat.strip() for kategori in kategoris_links for kat in kategori.split('|') if kat.strip() != '']\n",
    "    \n",
    "    return kategoris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genetika molekular', 'nukleat asam dna', 'asam nukleat']\n"
     ]
    }
   ],
   "source": [
    "with open('pages.csv', 'r', encoding='UTF-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    \n",
    "    next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "        print(get_kategoris(row))\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melakukan indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class BSBIIndex:\n",
    "    def __init__(self, data_path, output_path, postings_encoding, index_name=\"main_index\"):\n",
    "        self.cat_id_map = IdMap()\n",
    "        self.page_id_map = IdMap()\n",
    "        self.data_path = data_path\n",
    "        self.output_path = output_path\n",
    "        self.index_name = index_name\n",
    "        self.postings_encoding = postings_encoding\n",
    "\n",
    "        # Untuk menyimpan nama-nama file dari semua incatediate inverted index\n",
    "        self.incatediate_indices = []\n",
    "\n",
    "        # Untuk menyimpan data waktu running\n",
    "        self.log = []\n",
    "\n",
    "    def save(self):\n",
    "        with open('categories.csv', 'w', encoding='UTF-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            for key, value in enumerate(self.cat_id_map.id_to_str):\n",
    "                writer.writerow([key, value])\n",
    "        \n",
    "        with open(os.path.join(self.output_path, 'cats.dict'), 'wb') as f:\n",
    "            pickle.dump(self.cat_id_map, f)\n",
    "        with open(os.path.join(self.output_path, 'pages.dict'), 'wb') as f:\n",
    "            pickle.dump(self.page_id_map, f)\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"Memuat page_id_map and cat_id_map dari output directory\"\"\"\n",
    "\n",
    "        with open(os.path.join(self.output_path, 'cats.dict'), 'rb') as f:\n",
    "            self.cat_id_map = pickle.load(f)\n",
    "        with open(os.path.join(self.output_path, 'pages.dict'), 'rb') as f:\n",
    "            self.page_id_map = pickle.load(f)\n",
    "\n",
    "    def start_indexing(self):\n",
    "    # loop untuk setiap sub-directory di dalam folder collection (setiap block)\n",
    "        start_time = time.time()\n",
    "        \n",
    "        cd_pairs = self.parsing_pages()\n",
    "        # print(f'cd_pairs {cd_pairs}')\n",
    "        \n",
    "        with InvertedIndexWriter(self.index_name, self.postings_encoding, path=self.output_path) as main_index:\n",
    "            self.write_to_index(cd_pairs, main_index)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        self.log.append(f'Total waktu keseluruhan encoding dengan {self.postings_encoding} adalah {round((end_time - start_time) / 60, 8)}')\n",
    "        self.save()\n",
    "\n",
    "    def parsing_pages(self):\n",
    "        res = []\n",
    "        with open(self.data_path, 'r', newline='', encoding='UTF-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            \n",
    "            next(reader)\n",
    "            \n",
    "            counter = 0\n",
    "            \n",
    "            for row in tqdm(reader, desc=\"Parsing pages\"):\n",
    "                page_id = self.page_id_map[row[0]]\n",
    "                kategoris = get_kategoris(row)\n",
    "                for kategori in kategoris:\n",
    "                    res.append((self.cat_id_map[kategori], page_id))\n",
    "                    \n",
    "                # counter += 1\n",
    "                # if counter > 1e3:\n",
    "                #     break\n",
    "\n",
    "        return res\n",
    "\n",
    "    def write_to_index(self, cd_pairs, index):\n",
    "        cat_dict = {}\n",
    "        for cat_id, page_id in tqdm(cd_pairs, desc=\"Writing to index\"):\n",
    "            if cat_id not in cat_dict:\n",
    "                cat_dict[cat_id] = set()\n",
    "            cat_dict[cat_id].add(page_id)\n",
    "        for cat_id in sorted(cat_dict.keys()):\n",
    "            index.append(cat_id, sorted(list(cat_dict[cat_id])))\n",
    "\n",
    "    def boolean_retrieve(self, kategoris):\n",
    "        # TODO\n",
    "\n",
    "        self.load()\n",
    "        res = []\n",
    "\n",
    "        start_time = time.time()\n",
    "        with InvertedIndexReader(self.index_name, self.postings_encoding, path=self.output_path) as indices:\n",
    "            for kategori in kategoris:\n",
    "                # print(kategori)\n",
    "                kategori = kategori.lower()\n",
    "                if kategori not in self.cat_id_map:\n",
    "                    continue\n",
    "\n",
    "                cat_id = self.cat_id_map[kategori]\n",
    "                \n",
    "                # print(cat_id)\n",
    "                # print(f'sebelum {res}')\n",
    "\n",
    "                if not res:\n",
    "                    res = indices.get_postings_list(cat_id)\n",
    "                else:\n",
    "                    res = sort_union_list(res, indices.get_postings_list(cat_id))\n",
    "                    \n",
    "                # print(f'setelah {res}')\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        # print(\n",
    "        #     f'Waktu untuk melakukan query {kategori} dengan encoding {self.postings_encoding} sampai mendapatkan hasil adalah {duration} s')\n",
    "\n",
    "        return [self.page_id_map[page_id] for page_id in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "\n",
    "maxInt = sys.maxsize\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        csv.field_size_limit(maxInt)\n",
    "        break\n",
    "    except OverflowError:\n",
    "        maxInt = int(maxInt/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing pages: 1660717it [2:02:56, 225.13it/s] \n",
      "Writing to index: 100%|██████████| 1677886/1677886 [00:03<00:00, 430748.19it/s]\n"
     ]
    }
   ],
   "source": [
    "BSBI_instance = BSBIIndex(data_path='pages.csv', \\\n",
    "                               postings_encoding=VBEPostings,\n",
    "                               output_path='index')\n",
    "\n",
    "BSBI_instance.start_indexing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['genetika molekular', 'nukleat asam dna', 'asam nukleat']\n",
      "[]\n",
      "['presiden mesir', 'perdana menteri mesir', 'pemenang hadiah nobel perdamaian', 'kepala negara yang dibunuh', 'person of the year', 'pemimpin perang dingin', 'tokoh militer mesir', 'kolonel']\n",
      "['tokoh yang tidak memiliki informasi tahun kelahiran']\n",
      "['arkeologi']\n",
      "['antropologi']\n",
      "['ilmu komputer']\n",
      "['bahasa indonesia', 'bahasa di indonesia', 'bahasa di asia', 'bahasa di timor leste', 'bahasa di asia tenggara', 'bahasa berpola subjek–predikat–objek', 'rumpun bahasa austronesia', 'rumpun bahasa melayik']\n",
      "['biologi']\n",
      "['bali', 'provinsi di indonesia', 'pulau di indonesia', 'kepulauan sunda kecil', 'kepulauan sunda', 'pendirian tahun 1958 di indonesia', 'negara dan wilayah yang didirikan tahun 1958']\n",
      "['rumpun bahasa austronesia', 'bahasa di indonesia', 'bahasa yang mempunyai aksara tersendiri']\n"
     ]
    }
   ],
   "source": [
    "with open('pages.csv', 'r', encoding='UTF-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    \n",
    "    next(reader)\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    for row in reader:\n",
    "        print(get_kategoris(row))\n",
    "        counter += 1\n",
    "        \n",
    "        if counter > 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "136\n",
      "487\n",
      "2303\n",
      "2319\n",
      "2337\n",
      "2367\n",
      "2411\n",
      "2499\n",
      "2856\n",
      "2869\n",
      "4793\n",
      "5101\n",
      "5234\n",
      "6062\n",
      "8013\n",
      "9906\n",
      "9971\n",
      "9983\n",
      "10519\n",
      "10980\n",
      "12293\n",
      "12307\n",
      "14094\n",
      "17268\n",
      "17895\n",
      "18488\n",
      "18609\n",
      "20743\n",
      "21808\n",
      "21812\n",
      "22235\n",
      "23198\n",
      "23199\n",
      "23200\n",
      "23208\n",
      "24298\n",
      "24772\n",
      "25227\n",
      "27016\n",
      "28882\n",
      "29230\n",
      "29232\n",
      "29362\n",
      "29664\n",
      "29678\n",
      "29721\n",
      "30414\n",
      "31560\n",
      "32453\n",
      "33986\n",
      "34922\n",
      "39618\n",
      "40048\n",
      "40640\n",
      "41228\n",
      "42557\n",
      "42603\n",
      "42607\n",
      "42764\n",
      "44858\n",
      "46288\n",
      "46315\n",
      "46319\n",
      "46353\n",
      "46355\n",
      "46362\n",
      "65872\n",
      "72371\n",
      "73256\n",
      "75013\n",
      "76915\n",
      "79154\n",
      "82770\n",
      "97254\n",
      "98147\n",
      "99362\n",
      "101584\n",
      "101684\n",
      "107409\n",
      "107412\n",
      "114450\n",
      "118499\n",
      "126354\n",
      "128443\n",
      "129180\n",
      "129182\n",
      "145073\n",
      "145075\n",
      "161622\n",
      "163637\n",
      "164257\n",
      "165082\n",
      "165353\n",
      "166579\n",
      "169982\n",
      "174787\n",
      "174967\n",
      "175013\n",
      "175466\n",
      "175470\n",
      "175531\n",
      "175740\n",
      "184902\n",
      "194669\n",
      "197658\n",
      "197687\n",
      "205008\n",
      "261439\n",
      "311888\n",
      "331610\n",
      "366608\n",
      "366736\n",
      "366744\n",
      "373670\n",
      "400861\n",
      "407212\n",
      "429238\n",
      "510724\n",
      "510732\n",
      "513240\n",
      "514806\n",
      "514848\n",
      "515064\n",
      "516271\n",
      "518489\n",
      "525100\n",
      "667138\n",
      "704993\n",
      "708818\n",
      "715759\n",
      "717646\n",
      "721518\n",
      "731068\n",
      "732710\n",
      "734454\n",
      "746148\n",
      "746169\n",
      "746985\n",
      "747134\n",
      "747193\n",
      "747324\n",
      "748213\n",
      "750090\n",
      "751481\n",
      "756379\n",
      "758715\n",
      "758729\n",
      "760812\n",
      "811030\n",
      "813944\n",
      "970180\n",
      "1014514\n",
      "1015944\n",
      "1016691\n",
      "1016907\n",
      "1021647\n",
      "1023562\n",
      "1023576\n",
      "1091167\n",
      "1096011\n",
      "1096324\n",
      "1096369\n",
      "1100478\n",
      "1101098\n",
      "1107090\n",
      "1127634\n",
      "1144285\n",
      "1150584\n",
      "1210329\n",
      "1215931\n",
      "1240305\n",
      "1247561\n",
      "1264481\n",
      "1308668\n",
      "1331638\n",
      "1396910\n",
      "1399314\n",
      "1409163\n",
      "1494572\n",
      "1496547\n",
      "1503567\n",
      "1573937\n",
      "1575766\n",
      "1640926\n"
     ]
    }
   ],
   "source": [
    "BSBI_instance = BSBIIndex(data_path='pages.csv', \\\n",
    "                               postings_encoding=VBEPostings,\n",
    "                               output_path='index')\n",
    "\n",
    "kategoris = ['orde baru', 'tokoh orde baru']\n",
    "results = []\n",
    "\n",
    "for page in BSBI_instance.boolean_retrieve(kategoris):\n",
    "    print(page)\n",
    "    results.append(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamengkubuwana IX\n",
      "Soeharto\n",
      "Bob Hasan\n",
      "Siti Hardijanti Rukmana\n",
      "Siti Hartinah\n",
      "Komando Operasi Pemulihan Keamanan dan Ketertiban\n",
      "Partai Golongan Karya\n",
      "Try Sutrisno\n",
      "Abdul Haris Nasution\n",
      "Basuki Rahmat\n",
      "R. Hartono\n",
      "Akbar Tanjung\n",
      "Soedharmono\n",
      "Wiranto\n",
      "Gerakan 30 September\n",
      "Tragedi Trisakti\n",
      "Orde Baru\n",
      "Kerusuhan Mei 1998\n",
      "Harmoko\n",
      "Penumpasan Pengkhianatan G 30 S PKI\n",
      "Kelompencapir\n",
      "Tommy Soeharto\n",
      "Radius Prawiro\n",
      "Ali Moertopo\n",
      "Probosutedjo\n",
      "Arief Budiman\n",
      "Sudono Salim\n",
      "Ibnu Sutowo\n",
      "Peristiwa 27 Juli\n",
      "Malari\n",
      "Pembantaian Santa Cruz\n",
      "Ismail Saleh\n",
      "Ali Said\n",
      "Soegih Arto\n",
      "Andi Muhammad Ghalib\n",
      "Soesilo Soedarman\n",
      "Rencana Pembangunan Lima Tahun\n",
      "Mochtar Lubis\n",
      "Transmigrasi\n",
      "Dorodjatun Kuntjoro-Jakti\n",
      "Maraden Panggabean\n",
      "M. Jusuf\n",
      "Leonardus Benyamin Moerdani\n",
      "Feisal Tanjung\n",
      "Syarwan Hamid\n",
      "Arifin Siregar\n",
      "Abdul Latief (pengusaha)\n",
      "Amir Machmud\n",
      "J. B. Sumarlin\n",
      "Mafia Berkeley\n",
      "Penentuan Pendapat Rakyat\n",
      "Achmad Tahir\n",
      "Kasus dugaan korupsi Soeharto\n",
      "De-Soekarnoisasi\n",
      "Titiek Soeharto\n",
      "Cosmas Batubara\n",
      "Alamsyah Ratu Perwiranegara\n",
      "Bustanil Arifin\n",
      "Trilogi Pembangunan\n",
      "Daoed Joesoef\n",
      "Sudomo\n",
      "Frans Seda\n",
      "Soemitro Djojohadikoesoemo\n",
      "Soemitro\n",
      "Idham Chalid\n",
      "Fuad Bawazier\n",
      "Mukti Ali\n",
      "Mashuri Saleh\n",
      "Hartono Rekso Dharsono\n",
      "Japto Soerjosoemarno\n",
      "Subroto\n",
      "Soedjatmoko\n",
      "Alex Noerdin\n",
      "Arie Jeffry Kumaat\n",
      "Sarwo Edhie Wibowo\n",
      "Pendudukan Gedung DPR/MPR\n",
      "Haryono Suyono\n",
      "Kategori:Panglima Komando Keamanan dan Ketertiban\n",
      "Jusuf Wanandi\n",
      "Sofjan Wanandi\n",
      "Theo L. Sambuaga\n",
      "Nugroho Notosusanto\n",
      "Invasi Indonesia ke Timor Timur\n",
      "Mohammad Sadli\n",
      "Oemar Seno Adji\n",
      "Moerdiono\n",
      "Widjojo Nitisastro\n",
      "Ali Wardhana\n",
      "Pendudukan Indonesia di Provinsi Timor Timur\n",
      "Petisi 50\n",
      "Peristiwa Talangsari 1989\n",
      "Yoga Sugama\n",
      "Joop Ave\n",
      "Soekarmen\n",
      "Kategori:Pendudukan Indonesia di Timor Timur\n",
      "Boediardjo\n",
      "Sumantri Brodjonegoro\n",
      "Oetojo Oesman\n",
      "Soedjarwo\n",
      "G.A. Siwabessy\n",
      "Syarief Thayeb\n",
      "Rachmat Saleh\n",
      "Sintong Panjaitan\n",
      "Toyib Hadiwijaya\n",
      "Soedarsono Hadisapoetro\n",
      "Sutami\n",
      "Soerjadi (hakim)\n",
      "Penembakan misterius\n",
      "Kemal Idris\n",
      "M. Sarbini\n",
      "Kerusuhan Situbondo\n",
      "Kejatuhan Soeharto\n",
      "Kepresidenan Sementara Soeharto\n",
      "Kerusuhan Banjarmasin\n",
      "Muhammad Dahlan\n",
      "Pemberdayaan Kesejahteraan Keluarga\n",
      "Amir Murtono\n",
      "Bambang Trihatmodjo\n",
      "Sigit Harjojudanto\n",
      "Operasi militer Indonesia di Aceh 1990-1998\n",
      "Tragedi Simpang KKA\n",
      "Tragedi Idi Cut\n",
      "Kategori:Penandatangan Petisi 50\n",
      "Tragedi Jambo Keupok\n",
      "Sejarah Indonesia (1965–1966)\n",
      "Amran YS\n",
      "Aspri\n",
      "Mohammad Zamroni\n",
      "Tragedi Gedung KNPI Aceh Utara\n",
      "Setya Novanto\n",
      "Mohammad Syafa'at Mintaredja\n",
      "Harsono Tjokroaminoto\n",
      "Pembantaian Beutong Ateueh\n",
      "Suhardiman\n",
      "Witarmin\n",
      "Kategori:Tokoh Orde Baru\n",
      "Kategori:Perserikatan Orde Baru\n",
      "Kategori:Seniman Orde Baru\n",
      "Kategori:Propaganda Orde Baru\n",
      "Kategori:Donatur Orde Baru\n",
      "Kategori:Paramiliter Orde Baru\n",
      "Osa Maliki Wangsadinata\n",
      "Tefaat\n",
      "Kategori:Tokoh Orde Lama\n",
      "Sunawar Sukowati\n",
      "Tragedi Relawan LSM RATA\n",
      "Tragedi Peudada\n",
      "Rusli Noor\n",
      "Tahun Kunjungan Indonesia\n",
      "Soegiarto (militer)\n",
      "Budi pekerti\n",
      "Proyek lahan gambut satu juta hektar\n",
      "Kasus biskuit beracun\n",
      "Klobotisme\n",
      "Paket Kebijaksanaan Oktober 1988\n",
      "Pembantaian Purwodadi\n",
      "Kebijakan 15 November 1978\n",
      "Siswadji\n",
      "Endang Kusuma Inten Soeweno\n",
      "Adhyatma\n",
      "Nasrudin Sumintapura\n",
      "Bernardus Sugiarta Muljana\n",
      "Sapardjo\n",
      "Genosida Timor Timur\n",
      "Sugiyono (politikus)\n",
      "Ipik Asmasoebrata\n",
      "Bapakisme\n",
      "Tebu Rakyat Intensifikasi\n",
      "Yogi Supardi\n",
      "Pendidikan Moral Pancasila\n",
      "Widjojo Soejono\n",
      "Hario Jonosewojo\n",
      "Ibuisme negara\n",
      "Suyono Sosrodarsono\n",
      "Kelompok Empat (Indonesia)\n",
      "Badan Pembinaan Pendidikan Pelaksanaan Pedoman Penghayatan dan Pengamalan Pancasila\n",
      "Umar Rukman\n",
      "Sutanto Wiryoprasonto\n",
      "Bakorperagon\n",
      "Badan Penyangga dan Pemasaran Cengkeh\n",
      "Jonggol sebagai Kandidat Ibukota Indonesia\n",
      "Kamp pengasingan Moncongloe\n",
      "Kategori:Kamp konsentrasi Moncongloe\n",
      "Normalisasi Kehidupan Kampus/Badan Koordinasi Kemahasiswaan\n"
     ]
    }
   ],
   "source": [
    "with open('pages.csv', 'r', encoding='UTF-8', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    \n",
    "    next(reader)\n",
    "    \n",
    "    max_id = max(list(map(int, results)))\n",
    "    \n",
    "    for row in reader:\n",
    "        if int(row[0]) > max_id:\n",
    "            break\n",
    "        \n",
    "        if row[0] in results:\n",
    "            print(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "184"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "\n",
    "with open('infobox_pages.csv', 'r', encoding='UTF-8', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    \n",
    "    next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "\n",
    "with open('infoboxless_pages.csv', 'r', encoding='UTF-8', newline='') as file:\n",
    "    reader = csv.reader(file)\n",
    "    \n",
    "    next(reader)\n",
    "    \n",
    "    for row in reader:\n",
    "        total += 1\n",
    "\n",
    "total"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
